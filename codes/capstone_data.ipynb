{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "I have decided to go with my second choice, which is to predict the price of HDB resale flats. If time permits I will also predict the price private residential properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "For most people, purchasing a property is their biggest investment and so it is important for buyers to know if they can afford the downpayment and the mortage payment. In Singapore, the government gives generous subsidies for first-time home buyers of public housing flats built by Housing and Development Board (HDB). Singaporeans that meet the criteria (there is an income ceiling) took advantage of this policy to own their homes, resulted in 80% of the residents living in HDB flats.\n",
    "\n",
    "The private property market is for local high income earners and HDB flat upgraders. Many foreigners also see Singapore as a safe haven to park thier monies here.\n",
    "\n",
    "The price of a property is determined by many factors. Beside the condition, size and location of the house, it is also affected by the state of the economy, government policy and supply and demand. \n",
    "\n",
    "I will compare different regression models to predict the prices of HDB flats, based on the propertys' attributes, their proximity to points of interest, market supply and demand and macroeconomic factors. \n",
    "\n",
    "I will be using RMSE to measure model performance, and the model should at least improve upon baseline by 10%. Baseline is defined as the average of property prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risks and Assumptions\n",
    "\n",
    "**May not be able to fetch Geocodes for all HDB flats**\n",
    "\n",
    "I have tried using Google Map and Nominatim to get geocodes of HDB flats and points of interest. \n",
    "\n",
    "Google Map is able to get geocodes for all physical addresses and but Google charges USD7 per 1000 requests for the first 100,000 requests. \n",
    "\n",
    "Nominatim is FREE but is unable to get geocodes for about 15% of the addresses. In order to mitigate this risk I will collect more HDB resale transaction data and drop those observations that it failed to get geocodes. Alternatively I will pay for Google service, use manual method or explore other free geocoding services. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopy.geocoders\n",
    "from geopy.geocoders import Nominatim, GoogleV3\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from geopy.exc import GeocoderServiceError\n",
    "from geopy.exc import GeopyError\n",
    "import scipy.stats as sps\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from matplotlib import pyplot\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File organization\n",
    " - notebook in codes folder\n",
    " - data files to be imported in datasets/input folder\n",
    " - data files exported in datasets/output folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection, Cleaning and Munging\n",
    "\n",
    "Types of Data collected\n",
    "\n",
    "1.HDB Flat Resale transactions (2017 to 2020)\n",
    "- flat attributes and prices\n",
    "\n",
    "2.Supply and Demand Factors\n",
    "- Supply of new HDB flats\n",
    "- Supply of new private properties\n",
    "- population size\n",
    "- number of married people\n",
    "\n",
    "3.Macroeconomic factors\n",
    "- Consumer Price Index\n",
    "- Purchasing Manager Index\n",
    "- Composite Leading Index\n",
    "- GDP Growth\n",
    "- CPF interest rates\n",
    "- Singapore Interbank Offered Rate (SIBOR)\n",
    "- umemployment rate\n",
    "- median income of residents\n",
    "\n",
    "4.Cost Factors\n",
    "- HDB flat price index\n",
    "- Private property pric index\n",
    "\n",
    "5.Points of Interest\n",
    "- Shopping Malls (more malls to be manually added)\n",
    "- Nature Parks\n",
    "- Columbaria/crematoria/cemeteries\n",
    "- Schools\n",
    "- Sports Facitilites\n",
    "- MRT/LRT stations\n",
    "- Hawker centres and markets\n",
    "\n",
    "\n",
    "## Progress\n",
    "State of Data collection - 99%<br/>\n",
    "State of Data Munging - 99%<br/>\n",
    "State of EDA - 70%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HDB Flat Resale Transactions\n",
    "\n",
    "Souce: data.gov.sg<br/>\n",
    "Observation: per sale transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>town</th>\n",
       "      <th>flat_type</th>\n",
       "      <th>block</th>\n",
       "      <th>street_name</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>flat_model</th>\n",
       "      <th>lease_commence_date</th>\n",
       "      <th>remaining_lease</th>\n",
       "      <th>resale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70874</th>\n",
       "      <td>2020-05</td>\n",
       "      <td>YISHUN</td>\n",
       "      <td>5 ROOM</td>\n",
       "      <td>342A</td>\n",
       "      <td>YISHUN RING RD</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>117.0</td>\n",
       "      <td>Premium Apartment</td>\n",
       "      <td>2016</td>\n",
       "      <td>94 years 09 months</td>\n",
       "      <td>578000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70875</th>\n",
       "      <td>2020-05</td>\n",
       "      <td>YISHUN</td>\n",
       "      <td>5 ROOM</td>\n",
       "      <td>335C</td>\n",
       "      <td>YISHUN ST 31</td>\n",
       "      <td>13 TO 15</td>\n",
       "      <td>112.0</td>\n",
       "      <td>Improved</td>\n",
       "      <td>2015</td>\n",
       "      <td>94 years 06 months</td>\n",
       "      <td>550000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70876</th>\n",
       "      <td>2020-05</td>\n",
       "      <td>YISHUN</td>\n",
       "      <td>5 ROOM</td>\n",
       "      <td>505A</td>\n",
       "      <td>YISHUN ST 51</td>\n",
       "      <td>13 TO 15</td>\n",
       "      <td>112.0</td>\n",
       "      <td>Improved</td>\n",
       "      <td>2016</td>\n",
       "      <td>94 years 11 months</td>\n",
       "      <td>540000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70877</th>\n",
       "      <td>2020-05</td>\n",
       "      <td>YISHUN</td>\n",
       "      <td>EXECUTIVE</td>\n",
       "      <td>391</td>\n",
       "      <td>YISHUN AVE 6</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>142.0</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>1988</td>\n",
       "      <td>67 years 02 months</td>\n",
       "      <td>553000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70878</th>\n",
       "      <td>2020-05</td>\n",
       "      <td>YISHUN</td>\n",
       "      <td>EXECUTIVE</td>\n",
       "      <td>837</td>\n",
       "      <td>YISHUN ST 81</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>145.0</td>\n",
       "      <td>Maisonette</td>\n",
       "      <td>1988</td>\n",
       "      <td>66 years 09 months</td>\n",
       "      <td>652888.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         month    town  flat_type block     street_name storey_range  \\\n",
       "70874  2020-05  YISHUN     5 ROOM  342A  YISHUN RING RD     10 TO 12   \n",
       "70875  2020-05  YISHUN     5 ROOM  335C    YISHUN ST 31     13 TO 15   \n",
       "70876  2020-05  YISHUN     5 ROOM  505A    YISHUN ST 51     13 TO 15   \n",
       "70877  2020-05  YISHUN  EXECUTIVE   391    YISHUN AVE 6     10 TO 12   \n",
       "70878  2020-05  YISHUN  EXECUTIVE   837    YISHUN ST 81     01 TO 03   \n",
       "\n",
       "       floor_area_sqm         flat_model  lease_commence_date  \\\n",
       "70874           117.0  Premium Apartment                 2016   \n",
       "70875           112.0           Improved                 2015   \n",
       "70876           112.0           Improved                 2016   \n",
       "70877           142.0          Apartment                 1988   \n",
       "70878           145.0         Maisonette                 1988   \n",
       "\n",
       "          remaining_lease  resale_price  \n",
       "70874  94 years 09 months      578000.0  \n",
       "70875  94 years 06 months      550000.0  \n",
       "70876  94 years 11 months      540000.0  \n",
       "70877  67 years 02 months      553000.0  \n",
       "70878  66 years 09 months      652888.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdb_df = pd.read_csv('../datasets/input/resale-flat-prices-2017-2020.csv')\n",
    "hdb_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for any null value\n",
    "hdb_df.isna().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all text values to lowercase\n",
    "hdb_df['town'] = hdb_df['town'].str.lower()\n",
    "hdb_df['flat_type'] = hdb_df['flat_type'].str.lower()\n",
    "hdb_df['block'] = hdb_df['block'].str.lower()\n",
    "hdb_df['street_name'] = hdb_df['street_name'].str.lower()\n",
    "hdb_df['storey_range'] = hdb_df['storey_range'].str.lower()\n",
    "hdb_df['flat_model'] = hdb_df['flat_model'].str.lower()\n",
    "\n",
    "# combine block and street_name to create the address column, and drop the original columns\n",
    "hdb_df['address'] = hdb_df['block'] + ' ' + hdb_df['street_name']\n",
    "hdb_df.drop(['block','street_name'],axis=1,inplace=True)\n",
    "\n",
    "hdb_df[['year','month']] = hdb_df['month'].str.split(\"-\", 1, expand=True)\n",
    "\n",
    "hdb_df['year'] = hdb_df['year'].astype(np.int32)\n",
    "hdb_df['month'] = hdb_df['month'].astype(np.int32)\n",
    "\n",
    "hdb_df = hdb_df.reindex(['year', 'month', 'town', 'flat_type', 'storey_range', 'floor_area_sqm',\n",
    "       'flat_model', 'lease_commence_date', 'remaining_lease', 'resale_price',\n",
    "       'address', 'lat', 'lng'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many addresses in Singapore use abbreviations, which can affect the ability of geocoding services to get geocodes\n",
    "# The following will convert abbreviations in addresses to their original forms\n",
    "\n",
    "hdb_df.replace(regex={\n",
    "        r\"\\b(upp)\\b\":\"upper\",r\"\\b(rd)\\b\":\"road\",r\"\\b(lor)\\b\":\"lorong\",\n",
    "        r\"\\b(ave)\\b\":\"avenue\",r\"\\b(jln)\\b\":\"jalan\",r\"\\b(sth)\\b\":\"south\",r\"\\b(nth)\\b\":\"north\",\n",
    "        r\"\\b(ctrl)\\b\":\"central\",r\"\\b(blk)\\b\":\"block\",r\"\\b(blvd)\\b\":\"boluevard\",\n",
    "        r\"\\b(bt)\\b\":\"bukit\",r\"\\b(c'wealth)\\b\":\"commonwealth\",r\"\\b(cl)\\b\":\"close\",r\"\\b(cplx)\\b\":\"complex\",\n",
    "        r\"\\b(ctr)\\b\":\"centre\",r\"\\b(dr)\\b\":\"drive\",r\"\\b(est)\\b\":\"estate\",r\"\\b(gdn)\\b\":\"garden\",\n",
    "        r\"\\b(gdns)\\b\":\"gardens\",r\"\\b(gr)\\b\":\"grove\",r\"\\b(hse)\\b\":\"house\",r\"\\b(hts)\\b\":\"heights\",\n",
    "        r\"\\b(ind)\\b\":\"industrial\",r\"\\b(distripk)\\b\":\"distripark\",r\"\\b(intl)\\b\":\"international\",\n",
    "        r\"\\b(lk)\\b\":\"link\",r\"\\b(mkt)\\b\":\"market\",r\"\\b(mjd)\\b\":\"masjid\",r\"\\b(mt)\\b\":\"mount\",\n",
    "        r\"\\b(natl)\\b\":\"national\",r\"\\b(opp)\\b\":\"opposite\",r\"\\b(pk)\\b\":\"park\",r\"\\b(pl)\\b\":\"place\",\n",
    "        r\"\\b(pt)\\b\":\"point\",r\"\\b(resvr)\\b\":\"reservoir\",r\"\\b(sch)\\b\":\"school\",r\"\\b(sci)\\b\":\"science\",\n",
    "        r\"\\b(sq)\\b\":\"square\",r\"\\b(ter)\\b\":\"terrace\",r\"\\b(tg)\\b\":\"tanjong\",\n",
    "        r\"\\b(tp)\\b\":\"temple\",r\"\\b(twr)\\b\":\"tower\",r\"\\b(w'lands)\\b\":\"woodlands\",r\"\\b(wk)\\b\":\"walk\",\n",
    "        r\"\\b(wtr)\\b\":\"water\",r\"\\b(v)\\b\":\"village\",r\"\\b(veh)\\b\":\"vehicle\",r\"\\b(warehse)\\b\":\"warehouse\",\n",
    "        r\"\\b(bef)\\b\":\"before\",r\"\\b(aft)\\b\":\"after\",r\"\\b(svc)\\b\":\"service\",\n",
    "        r\"\\b(svcs)\\b\":\"services\",r\"\\b(sg)\\b\":\"sungei\",r\"\\b(kg)\\b\":\"kampong\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google Geocoding API to get geocodes\n",
    "\n",
    "# I have removed my Google API Key because anyone can use it to access Google services and charge to my account, \n",
    "# so the below code snippet is NOT Workable without the key\n",
    "\n",
    "\n",
    "def get_pos_by_name(location_name):\n",
    "    \n",
    "    geolocator = GoogleV3(api_key='#### Google API Key ######')\n",
    "    loc = geolocator.geocode(location_name, timeout=10)\n",
    "    if not loc:\n",
    "        return 0,0\n",
    "\n",
    "    return (loc.latitude, loc.longitude) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Nominatim to get geocodes is FREE but \n",
    "\n",
    "def get_lat_long(str_location):\n",
    "    \n",
    "    geolocator = Nominatim(user_agent='user_1')\n",
    "    geolocator.country_bias = 'SG'\n",
    "    \n",
    "\n",
    "    str_location = str_location.replace('\\n',', ')\n",
    "    str_location = str_location + \" Singapore\"\n",
    "    float_lat = 0\n",
    "    float_lon = 0\n",
    "    while float_lat==0 and len(str_location)>0:\n",
    "        try:\n",
    "            \n",
    "            location = geolocator.geocode(str_location, timeout=10, countries='singapore')  \n",
    "            # greatly reduce missing values with timeout   \n",
    "\n",
    "            float_lat, float_lon = location.latitude, location.longitude\n",
    "        except:\n",
    "            print('Sorry cannot get geocode for '+str_location)\n",
    "            break\n",
    "    \n",
    "    return float_lat, float_lon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation of lat and long take many hours. If I do it for all records at the same time and the computer will hang, so I will break them up by year.\n",
    "\n",
    "In order to avoid SettingWithCopyWarning, I will be using copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb_2019_df = hdb_df[hdb_df['year']==2019].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb_2020_df = hdb_df[hdb_df['year']==2020].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df_hdb(df,yr):\n",
    "  print('fetching lat/long and updating the dataframe .....')\n",
    "  end = len(df)\n",
    "  for i in range(end):\n",
    "    df.iloc[i,11], df.iloc[i,12] = get_lat_long(df.iloc[i,10])\n",
    "\n",
    "  path = '../datasets/output/hdb-resale-flat-prices-'+str(yr)+'.csv'\n",
    "  df.to_csv(path, index = False)\n",
    "\n",
    "  print('completed for year '+str(yr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update_df_hdb(hdb_2020_df,int(2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update_df_hdb(hdb_2019_df,int(2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supply and Demand Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observations in the datasets are monnthly, quarterly or yearly.\n",
    "# \n",
    "# Function to Expand quarterly figures to spread over the 3 months, to be used for datasets with quarterly records\n",
    "\n",
    "def qtr_2_mth(df_in):\n",
    "    df = df_in.copy(deep=True)\n",
    "    \n",
    "    size = len(df)\n",
    "    \n",
    "    for i in range(0,size*3,3):\n",
    "        for j in range(0,2):\n",
    "                \n",
    "            mth = df.iloc[i+j,1]+1\n",
    "\n",
    "            replica = pd.DataFrame({\"year\":int(df.iloc[i+j,0]),\"month\":mth,df.columns[2]:df.iloc[i+j,2]},index=[i+j+1])\n",
    "\n",
    "            df = pd.concat([df.iloc[:i+j+1],replica,df.iloc[i+j+1:]]).reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supply of private houses\n",
    "\n",
    "Source: data.gov.sg<br/>\n",
    "Observation: quarterly\n",
    "\n",
    "- The dataset includes completed and non-completed, which I will split them up into 2 dataframes - 1 for completed units and the other group for those in the pipeline.\n",
    "- Expand the quarterly figures to monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pte_status_df = pd.read_csv('../datasets/input/completion-status-of-private-residential.csv')\n",
    "pte_status_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 2 dataframes - 1 for completed houses, 1 for incomplete/planned\n",
    "\n",
    "pte_pipe_df = pte_status_df.loc[:,['quarter']].copy(deep=True)\n",
    "pte_pipe_df['pte_pipe'] = (pte_status_df['provisional_permission'] \n",
    "                        + pte_status_df['written_permission'] \n",
    "                        + pte_status_df['building_plan_approval'] \n",
    "                        + pte_status_df['building_commencement'])\n",
    "\n",
    "pte_built_df = pte_status_df.loc[:,['quarter']].copy(deep=True)\n",
    "pte_built_df['pte_built'] = pte_status_df['building_completion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the date field to year and quarter\n",
    "pte_pipe_df[['year','month']] = pte_pipe_df['quarter'].str.split(\"-\", 1, expand=True)\n",
    "pte_built_df[['year','month']] = pte_built_df['quarter'].str.split(\"-\", 1, expand=True)\n",
    "\n",
    "# convert quarter to the starting month of the quarter in numeric form \n",
    "pte_pipe_df = pte_pipe_df.replace('Q1',1).replace('Q2',4).replace('Q3',7).replace('Q4',10)\n",
    "pte_built_df = pte_built_df.replace('Q1',1).replace('Q2',4).replace('Q3',7).replace('Q4',10)\n",
    "\n",
    "# reorder the columns\n",
    "pte_pipe_df = pte_pipe_df.reindex(['year','month','pte_pipe'],axis=1)\n",
    "pte_built_df = pte_built_df.reindex(['year','month','pte_built'],axis=1)\n",
    "\n",
    "# change data type of numeric columns to integer\n",
    "pte_pipe_df['year'] = pte_pipe_df['year'].astype(np.int32)\n",
    "pte_pipe_df['month'] = pte_pipe_df['month'].astype(np.int32)\n",
    "pte_built_df['year'] = pte_built_df['year'].astype(np.int32)\n",
    "pte_built_df['month'] = pte_built_df['month'].astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT - DO NOT RUN MORE THAN ONCE\n",
    "\n",
    "pte_pipe_df = qtr_2_mth(pte_pipe_df)\n",
    "pte_built_df = qtr_2_mth(pte_built_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pte_built_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supply of HDB flats\n",
    "\n",
    "Source: data.gov.sg<br/>\n",
    "Observation: yearly\n",
    "\n",
    "The dataset includes completed and non-completed, which I will split them up into 2 dataframes - 1 for completed flats and the other group in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb_status_df = pd.read_csv('../datasets/input/completion-status-of-hdb-residential-developments.csv')\n",
    "hdb_status_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a list of HDB flats in the pipeline\n",
    "# Eligilibity criteria and other terms for DBSS are almost the same as normal HDB flats so they will be combined \n",
    "\n",
    "hdb_pipe_df =  hdb_status_df[hdb_status_df['status'] != 'Completed']\n",
    "\n",
    "# na means zero flats built for that year\n",
    "hdb_pipe_df = hdb_pipe_df.replace('na',0)\n",
    "\n",
    "# change datatype of numeric columns to integer \n",
    "hdb_pipe_df['no_of_units'] = hdb_pipe_df['no_of_units'].astype(np.int32)\n",
    "hdb_pipe_df['financial_year'] = hdb_pipe_df['financial_year'].astype(np.int32)\n",
    "\n",
    "# sum up DBSS and normal HDB flats\n",
    "hdb_pipe_df = hdb_pipe_df.groupby(['financial_year'])['no_of_units'].sum().reset_index()\n",
    "\n",
    "# rename columns\n",
    "hdb_pipe_df.rename(columns={'financial_year':'year','no_of_units':'hdb_pipe'},inplace=True)\n",
    "\n",
    "hdb_pipe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of HDB flats built\n",
    "# Eligilibity criteria and other terms for DBSS are almost the same as normal HDB flats so they will be combined \n",
    "\n",
    "hdb_built_df =  hdb_status_df[hdb_status_df['status']=='Completed']\n",
    "\n",
    "# na means zero flats built for that year\n",
    "hdb_built_df = hdb_built_df.replace('na',0)\n",
    "\n",
    "# change datatype of numeric columns to integer \n",
    "hdb_built_df['no_of_units'] = hdb_built_df['no_of_units'].astype(np.int32)\n",
    "hdb_built_df['financial_year'] = hdb_built_df['financial_year'].astype(np.int32)\n",
    "\n",
    "# sum up DBSS and normal HDB flats\n",
    "hdb_built_df = hdb_built_df.groupby(['financial_year'])['no_of_units'].sum().reset_index()\n",
    "\n",
    "# rename columns\n",
    "hdb_built_df.rename(columns={'financial_year':'year','no_of_units':'hdb_new'},inplace=True)\n",
    "\n",
    "hdb_built_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New HDB flats booked\n",
    "\n",
    "source: data.gov.sg<br/>\n",
    "observation: yearly\n",
    "\n",
    "This is another indicator of supply of flats. When buyers book a flat, they can expect to wait between a few months to a few years before they can move in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb_booked_df = pd.read_csv('../datasets/input/bookings-for-new-flats.csv')\n",
    "hdb_booked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change datatype of numeric columns to integer \n",
    "hdb_booked_df['no_of_units'] = hdb_booked_df['no_of_units'].astype(np.int32)\n",
    "hdb_booked_df['financial_year'] = hdb_booked_df['financial_year'].astype(np.int32)\n",
    "\n",
    "# Rename columns\n",
    "hdb_booked_df.rename(columns={'financial_year':'year','no_of_units':'new_hdb_booked'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population and types of home\n",
    "\n",
    "Source: singstat<br/>\n",
    "Observation: yearly\n",
    "\n",
    "I am mainly interested to find out if the population size affects house prices.\n",
    "\n",
    "I will also get the number of residents living in different types of houses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p_df = pd.read_csv('../datasets/input/population_singstat.csv')\n",
    "p_df.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve total residents and breakdown of residents for each type of house\n",
    "\n",
    "s1 = p_df.iloc[0:1,0:].T.iloc[1:,0]\n",
    "s2 = p_df.iloc[1:2,0:].T.iloc[1:,0]\n",
    "s3 = p_df.iloc[2:3,0:].T.iloc[1:,0]\n",
    "s4 = p_df.iloc[3:4,0:].T.iloc[1:,0]\n",
    "s5 = p_df.iloc[4:5,0:].T.iloc[1:,0]\n",
    "s6 = p_df.iloc[5:6,0:].T.iloc[1:,0]\n",
    "s7 = p_df.iloc[6:7,0:].T.iloc[1:,0]\n",
    "s8 = p_df.iloc[7:8,0:].T.iloc[1:,0]\n",
    "s9 = p_df.iloc[8:9,0:].T.iloc[1:,0]\n",
    "\n",
    "pop_df = pd.DataFrame()\n",
    "pop_df['year'] = s1.index\n",
    "pop_df['total_residents'] = s1.values\n",
    "pop_df['hdb_dwellers'] = s2.values\n",
    "pop_df['hdb_2r_dwellers'] = s3.values\n",
    "pop_df['hdb_3r_dwellers'] = s4.values\n",
    "pop_df['hdb_4r_dwellers'] = s5.values\n",
    "pop_df['hdb_5r_dwellers'] = s6.values\n",
    "pop_df['condo_dwellers'] = s7.values\n",
    "pop_df['landed_dwellers'] = s8.values\n",
    "pop_df['other_dwellers'] = s9.values\n",
    "pop_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Married People\n",
    "\n",
    "Source: singstat<br/>\n",
    "Observation: yearly\n",
    "\n",
    "Majority of HDB flats are purchased by married couples, so it would be interestingly to see how the number of married people affect flat prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_df = pd.read_csv('../datasets/input/married_singstat.csv')\n",
    "m_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transpose data from first row to a column\n",
    "\n",
    "s = m_df.iloc[2:3,0:].T.iloc[1:,0]\n",
    "\n",
    "married_df = pd.DataFrame()\n",
    "married_df['year'] = s.index\n",
    "married_df['married'] = s.values\n",
    "\n",
    "married_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove commmas from married\n",
    "married_df = married_df.replace(\",\",\"\",regex=True)\n",
    "\n",
    "# change datatype of numeric columns to integer \n",
    "married_df['year'] = married_df['year'].astype(np.int32)\n",
    "married_df['married'] = married_df['married'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "married_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Macroeconomic factors\n",
    "\n",
    "I will collect data from the Singapore government portals (data.gov.sg, singstat, SPIMM and associations of banks in singapore) such as CPF interest rates, Interbank interest rate, consumer price index, PMI, composite index, income, unemployment rates, GDP growth rates, etc\n",
    "\n",
    "The observatons are monthly, quarterly and yearly. I will expand the quarterly and yearly figures to monthly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite Leading Index (CLI)\n",
    "Source: data.gov.sg<br/>\n",
    "Observation: quarterly\n",
    "\n",
    "- Singapore's Composite Leading Index is used to anticipate the turning points of growth cycles, or fluctuations in the economy’s growth rate\n",
    "- Expand the quarterly figures to monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_df = pd.read_csv('../datasets/input/composite-leading-index.csv')\n",
    "cli_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the date field to year and quarter\n",
    "cli_df[['year','month']] = cli_df['quarter'].str.split(\"-\", 1, expand=True)\n",
    "\n",
    "# convert quarter to the starting month of the quarter in numeric form \n",
    "cli_df = cli_df.replace('Q1',1).replace('Q2',4).replace('Q3',7).replace('Q4',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column\n",
    "cli_df.rename(columns={\"value\":\"cli\"},inplace=True)\n",
    "\n",
    "# reorder the columns\n",
    "cli_df = cli_df.reindex(['year','month','cli'],axis=1)\n",
    "\n",
    "cli_df['year'] = cli_df['year'].astype(np.int32)\n",
    "cli_df['month'] = cli_df['month'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_df = qtr_2_mth(cli_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_df.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perchasing Manager Index (PMI)\n",
    "Source: Singapore Institute of Purchasing & Materials Management<br/>\n",
    "Observation: Monthly\n",
    "\n",
    "The PMI is an indicator of business activity - both in the manufacturing and services sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_df = pd.read_csv('../datasets/input/pmi_sipmm.csv')\n",
    "pmi_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the date field to year and month\n",
    "pmi_df[['month','year']] = pmi_df['Month/Year'].str.split(\"-\", 1, expand=True)\n",
    "\n",
    "# Rename column\n",
    "pmi_df.rename(columns={\"Singapore PMI\":\"pmi\"},inplace=True)\n",
    "\n",
    "# change month to numeric form\n",
    "dic = {'Jan':1,'Feb':2,'Mar':3,'Apr':4,'May':5,'Jun':6,'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12}\n",
    "pmi_df.month = pmi_df.month.map(dic)\n",
    "\n",
    "# change datatype of numeric columns to integer \n",
    "pmi_df['month'] = pmi_df['month'].astype(np.int32)\n",
    "pmi_df['year'] = pmi_df['year'].astype(np.int32)\n",
    "\n",
    "# change year to 4-digit format\n",
    "pmi_df['year'] = pmi_df['year']+2000\n",
    "\n",
    "# reorder the columns\n",
    "pmi_df = pmi_df.reindex(['year','month','pmi'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPF interest rates\n",
    "\n",
    "Source: data.gov.sg<br/>\n",
    "observation: monthly\n",
    "\n",
    "When you take a housing loan from HDB, you will enjoy a concessionary interest rate. This concessionary interest rate is pegged at 0.10% above the prevailing CPF Ordinary Account (OA) interest rate, and may be adjusted in January, April, July, and October, in line with CPF interest rate revisions.[Source: HDB](https://www.hdb.gov.sg/cs/infoweb/residential/servicing-your-hdb-loan/mortgage-loan/interest-rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CPF interest rates\n",
    "ci_df = pd.read_csv('../datasets/input/cpf-interest-rates.csv')\n",
    "\n",
    "# housing loan interest rate is based on Ordinary account\n",
    "cpf_df = ci_df[ci_df['account_type']=='Ordinary'].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the date field to year and quarter\n",
    "cpf_df[['year','month']] = cpf_df['mth'].str.split(\"-\", 1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpf_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing loan interest rate is based on Ordinary account\n",
    "cpf_df = ci_df[ci_df['account_type']=='Ordinary'].copy(deep=True)\n",
    "\n",
    "# split the date field to year and quarter\n",
    "cpf_df[['year','month']] = cpf_df['mth'].str.split(\"-\", 1, expand=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "cpf_df.rename(columns={\"interest_rate\":\"cpf_rate\"},inplace=True)\n",
    "\n",
    "# reorder the columns\n",
    "cpf_df = cpf_df.reindex(['year','month','cpf_rate'],axis=1)\n",
    "\n",
    "# change datatypes of numeric columns to integer/float\n",
    "cpf_df['year'] = cpf_df['year'].astype(np.int32)\n",
    "cpf_df['month'] = cpf_df['month'].astype(np.int32)\n",
    "cpf_df['cpf_rate'] = cpf_df['cpf_rate'].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpf_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpf_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer Price Index\n",
    "\n",
    "source: singstat<br/>\n",
    "observation: monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = pd.read_csv('../datasets/input/cpi_singstat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = c_df.iloc[0:1,0:].T.iloc[1:,0]\n",
    "\n",
    "cpi_df = pd.DataFrame()\n",
    "cpi_df['month'] = s.index\n",
    "cpi_df['cpi_index'] = s.values\n",
    "\n",
    "cpi_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the date field to year and quarter\n",
    "cpi_df[['year','month']] = cpi_df['month'].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# change months to numeric form\n",
    "dic = {'Jan':1,'Feb':2,'Mar':3,'Apr':4,'May':5,'Jun':6,'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12}\n",
    "cpi_df.month = cpi_df.month.map(dic)\n",
    "\n",
    "# rearrange columns\n",
    "cpi_df = cpi_df.reindex(['year','month','cpi_index'],axis=1)\n",
    "\n",
    "# change datatypes of numeric columns to integer\n",
    "cpi_df['year'] = cpi_df['year'].astype(np.int32)\n",
    "cpi_df['month'] = cpi_df['month'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GDP Growth Rates\n",
    "\n",
    "source: singstat<br/>\n",
    "observation: quarterly\n",
    "\n",
    "- Expand the quarterly figures to monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_df = pd.read_csv('../datasets/input/gdp_growth_singstat.csv')\n",
    "g_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = g_df.iloc[0:1,0:].T.iloc[1:,0]\n",
    "\n",
    "gdp_df = pd.DataFrame()\n",
    "gdp_df['month'] = s.index\n",
    "gdp_df['gdp_growth'] = s.values\n",
    "\n",
    "gdp_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the date field to year and quarter\n",
    "gdp_df[['year','month']] = gdp_df['month'].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# convert quarter to the starting month of the quarter in numeric form \n",
    "gdp_df = gdp_df.replace('1Q',1).replace('2Q',4).replace('3Q',7).replace('4Q',10)\n",
    "\n",
    "# reorder the columns\n",
    "gdp_df = gdp_df.reindex(['year','month','gdp_growth'],axis=1)\n",
    "\n",
    "gdp_df['year'] = gdp_df['year'].astype(np.int32)\n",
    "gdp_df['month'] = gdp_df['month'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN more than once. Otherwise it will keep expanding the rows\n",
    "gdp_df = qtr_2_mth(gdp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unemployment Rate\n",
    "\n",
    "Source: singstat<br/>\n",
    "observation: quarterly\n",
    "\n",
    "- Expand the quarterly figures to monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_df = pd.read_csv('../datasets/input/unemployment_singstat.csv')\n",
    "u_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = u_df.iloc[0:1,0:].T.iloc[1:,0]\n",
    "\n",
    "unemployed_df = pd.DataFrame()\n",
    "unemployed_df['month'] = s.index\n",
    "unemployed_df['unemployed_rate'] = s.values\n",
    "\n",
    "unemployed_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the date field to year and quarter\n",
    "\n",
    "unemployed_df[['year','month']] = unemployed_df['month'].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# convert quarter to the starting month of the quarter in numeric form \n",
    "\n",
    "unemployed_df = unemployed_df.replace('1Q',1).replace('2Q',4).replace('3Q',7).replace('4Q',10)\n",
    "\n",
    "# reorder the columns\n",
    "unemployed_df = unemployed_df.reindex(['year','month','unemployed_rate'],axis=1)\n",
    "\n",
    "# change datatype of numeric columns to integer\n",
    "unemployed_df['year'] = unemployed_df['year'].astype(np.int32)\n",
    "unemployed_df['month'] = unemployed_df['month'].astype(np.int32)\n",
    "\n",
    "unemployed_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN more than once. Otherwise it will keep expanding the rows\n",
    "unemployed_df = qtr_2_mth(unemployed_df)\n",
    "\n",
    "unemployed_df.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIBOR \n",
    "\n",
    "Source: abs.org.sg<br/>\n",
    "Observation : business days\n",
    "\n",
    "- Housing loans are commonly pegged to SIBOR (Singapore Interbank Offered Rate). There are several types of SIBOR, and the common ones for housing loans are 1-month and 3-month.\n",
    "\n",
    "- Calculate average the rates for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sibor_df = pd.read_csv('../datasets/input/sibor_abs.csv')\n",
    "sibor_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the date to day, month, year\n",
    "sibor_df[['day','month','year']] = sibor_df['SIBOR DATE'].str.split(\"/\", 2, expand=True)\n",
    "\n",
    "# convert numeric columns to integer\n",
    "sibor_df['month'] = sibor_df['month'].astype(np.int32)\n",
    "sibor_df['year'] = sibor_df['year'].astype(np.int32)\n",
    "\n",
    "# store sibor_3m average in a temp dataframe\n",
    "\n",
    "df = sibor_df.groupby(['year','month'])['SIBOR 3M'].mean().reset_index()\n",
    "\n",
    "# calcalute average of sibor_1m by month\n",
    "\n",
    "sibor_df = sibor_df.groupby(['year','month'])['SIBOR 1M'].mean().reset_index()\n",
    "sibor_df\n",
    "\n",
    "# merge the 1m and 3m rates into the original dataframe\n",
    "sibor_df['sibor_3m'] = df['SIBOR 3M']\n",
    "\n",
    "# rename column to lowercase and replace space with underscore\n",
    "sibor_df.rename(columns={'SIBOR 1M':'sibor_1m'},inplace=True)\n",
    "\n",
    "sibor_df.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Income\n",
    "\n",
    "source: singstat<br/>\n",
    "observation: yearly\n",
    "\n",
    "\n",
    "I will get the median income of the population of each year and see how it affects house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_df = pd.read_csv('../datasets/input/monthly_income_singstat.csv')\n",
    "ic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transpose first row of dataframe to get median monthly income\n",
    "\n",
    "s = ic_df.iloc[0:1,0:].T.iloc[1:,0]\n",
    "\n",
    "income_df = pd.DataFrame()\n",
    "income_df['year'] = s.index\n",
    "income_df['income'] = s.values\n",
    "income_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a missing value for income in 2005\n",
    "# i will get the average of incomes for 2004 and 2006\n",
    "\n",
    "# remove commas from income\n",
    "income_df = income_df.replace(\",\",\"\",regex=True)\n",
    "\n",
    "# get the incomes \n",
    "inc_2004 = int(income_df[income_df['year']=='2004'].values[0][1])\n",
    "inc_2006 = int(income_df[income_df['year']=='2006'].values[0][1])\n",
    "\n",
    "# update dataframe with the average\n",
    "income_df.iloc[4,1] = (inc_2004+inc_2006)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datatype of numeric columns to integer \n",
    "income_df['year'] = income_df['year'].astype(np.int32)\n",
    "income_df['income'] = income_df['income'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cost Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDB Resale Price Index\n",
    "\n",
    "Source: singstat<br/>\n",
    "observation : quarterly\n",
    "\n",
    "- Expand the quarterly figures to monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_df = pd.read_csv('../datasets/input/hdb_price_index_singstat.csv')\n",
    "hi_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose first row of dataframe\n",
    "s = hi_df.iloc[0:1,0:].T.iloc[1:,0]\n",
    "hdb_index_df = pd.DataFrame()\n",
    "hdb_index_df['month'] = s.index\n",
    "hdb_index_df['hdb_index'] = s.values\n",
    "\n",
    "# split the month into year and month\n",
    "hdb_index_df[['year','month']] = hdb_index_df['month'].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# convert quarters to the first month of the quarter\n",
    "hdb_index_df = hdb_index_df.replace('1Q',1).replace('2Q',4).replace('3Q',7).replace('4Q',10)\n",
    "\n",
    "# convert numeric columns to integer\n",
    "hdb_index_df['year'] = hdb_index_df['year'].astype(np.int32)\n",
    "hdb_index_df['month'] = hdb_index_df['month'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the columns\n",
    "hdb_index_df = hdb_index_df.reindex(['year','month','hdb_index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN more than once. Otherwise it will keep expanding the rows\n",
    "hdb_index_df = qtr_2_mth(hdb_index_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb_index_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private property price index\n",
    "\n",
    "Source: data.gov.sg<br/>\n",
    "observation: quarterly\n",
    "\n",
    "- Expand the quarterly figures to monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_df = pd.read_csv('../datasets/input/private-residential-property-price-index.csv') \n",
    "pi_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into condo and all private residential\n",
    "\n",
    "condo_index_df = pi_df[pi_df['property_type']=='Non-Landed'].copy(deep=True)\n",
    "pte_index_df = pi_df[pi_df['property_type']=='All Residential'].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split quarter into year and month \n",
    "condo_index_df[['year','month']] = condo_index_df['quarter'].str.split(\"-\", 1, expand=True)\n",
    "pte_index_df[['year','month']] = pte_index_df['quarter'].str.split(\"-\", 1, expand=True)\n",
    "\n",
    "# drop unnecessary columns\n",
    "# condo_index_df.drop(columns=['quarter','property_type'],inplace=True)\n",
    "\n",
    "# rename column to meaningful name\n",
    "condo_index_df.rename(columns={\"index\":\"condo_index\"},inplace=True)\n",
    "pte_index_df.rename(columns={\"index\":\"pte_index\"},inplace=True)\n",
    "\n",
    "# preparing for expanding quarterly figures to monthly\n",
    "condo_index_df = condo_index_df.replace('Q1',1).replace('Q2',4).replace('Q3',7).replace('Q4',10)\n",
    "pte_index_df = pte_index_df.replace('Q1',1).replace('Q2',4).replace('Q3',7).replace('Q4',10)\n",
    "\n",
    "# convert numeric columns to integer\n",
    "condo_index_df['year'] = condo_index_df['year'].astype(np.int32)\n",
    "condo_index_df['month'] = condo_index_df['month'].astype(np.int32)\n",
    "pte_index_df['year'] = pte_index_df['year'].astype(np.int32)\n",
    "pte_index_df['month'] = pte_index_df['month'].astype(np.int32)\n",
    "\n",
    "# reorder the columns\n",
    "condo_index_df = condo_index_df.reindex(['year','month','condo_index'],axis=1)\n",
    "pte_index_df = pte_index_df.reindex(['year','month','pte_index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand quarterly figures to monthly\n",
    "# DO NOT RUN more than once. Otherwise it will keep expanding the rows\n",
    "condo_index_df = qtr_2_mth(condo_index_df)\n",
    "pte_index_df = qtr_2_mth(pte_index_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Points of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary and Secondary Schools\n",
    "\n",
    "Source: data.gov.sg\n",
    "\n",
    "- fetch the geocodes of schools based on their physical addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_df = pd.read_csv('../datasets/input/general-information-of-schools.csv')\n",
    "\n",
    "school_df = school_df[['school_name','address','mainlevel_code']].copy(deep=True)\n",
    "\n",
    "school_df['school_name'] = school_df['school_name'].str.lower()\n",
    "school_df['address'] = school_df['address'].str.lower()\n",
    "school_df['mainlevel_code'] = school_df['mainlevel_code'].str.lower()\n",
    "\n",
    "school_df['lat'] = 0\n",
    "school_df['lng'] = 0\n",
    "\n",
    "print('fetching lat/long and updating the dataframe .....')\n",
    "#end = len(school_df)\n",
    "#for i in range(end):\n",
    "#    school_df.iloc[i,3], school_df.iloc[i,4] = get_lat_long(school_df.iloc[i,1])\n",
    "\n",
    "#path = '../datasets/output/schools.csv'\n",
    "#school_df.to_csv(path, index = False)\n",
    "\n",
    "print('completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hawker Centres and Markets\n",
    "\n",
    "Source: data.gov.sg\n",
    "\n",
    "- fetch the geocodes of hawker centres based on their physical addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df = pd.read_csv('../datasets/input/list-of-government-markets-hawker-centres.csv')\n",
    "hc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df = hc_df[['name_of_centre','location_of_centre','type_of_centre']].copy(deep=True)\n",
    "\n",
    "hc_df.rename(columns={'name_of_centre':'hawker_centre','location_of_centre':'address'},inplace=True)\n",
    "\n",
    "hc_df['lat'] = 0\n",
    "hc_df['lng'] = 0\n",
    "\n",
    "hc_df.columns\n",
    "\n",
    "print('fetching lat/long and updating the dataframe .....')\n",
    "#end = len(hc_df)\n",
    "#for i in range(end):\n",
    "#    hc_df.iloc[i,3], hc_df.iloc[i,4] = get_lat_long(hc_df.iloc[i,1])\n",
    "\n",
    "#path = '../datasets/output/hawker_centre_market.csv'\n",
    "#hc_df.to_csv(path, index = False)\n",
    "\n",
    "print('completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRT and LRT Stations\n",
    "\n",
    "Source: https://www.kaggle.com/yxlee245/singapore-train-station-coordinates?select=mrt_lrt_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = pd.read_csv('../datasets/input/datasets_287088_590207_mrt_lrt_data.csv')\n",
    "\n",
    "station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shopping Malls\n",
    "\n",
    "Source: wikipedia\n",
    "\n",
    "- use Beautiful Soup to scrap the data from web page\n",
    "- fetch geocodes based on mall's names (only work with Google geocoding service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the requests library to get the html from the home page\n",
    "res = requests.get('https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore')\n",
    "\n",
    "# Create a soup object from the html\n",
    "soup = bs(res.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malls = soup.find('div',{'class':'mw-parser-output'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe\n",
    "df_malls = pd.DataFrame(columns=['mall','lat','lng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_malls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each <a> tag to get mall's name and get its geocode\n",
    "# The codes below will NOT run successfullly because I have removed the Google API key from the function\n",
    "# Only Google service can get the geocodes of the malls. The free Nominatim failed to get any geocode for malls\n",
    "\n",
    "#i = 0\n",
    "#for m in malls.find_all('a', {'class': 'new'}):\n",
    "#    print(m.text)\n",
    "    \n",
    "#    lat, lng = get_pos_by_name(m.text + ' Singapore')\n",
    "    \n",
    "#    df_malls.loc[i] = [m.text,lat,lng]\n",
    "    \n",
    "#    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_malls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_malls.to_csv('../datasets/output/malls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nature Parks\n",
    "\n",
    "Source: data.gov.sg\n",
    "\n",
    "- scrape the data from the geojson file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe for nature parks\n",
    "df_parks = pd.DataFrame(columns=['park','lat','lng'])\n",
    "\n",
    "with open('../datasets/input/parks-geojson.geojson') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "i=0\n",
    "for feature in data['features']:\n",
    "    \n",
    "    soup = BeautifulSoup(feature['properties']['Description'], 'lxml')\n",
    "    \n",
    "    table = soup.find('table')\n",
    "       \n",
    "    for line in table.findAll('tr'):\n",
    "        \n",
    "        name = re.findall(r\"\\bNAME\\b\",line.getText())\n",
    "        \n",
    "        if len(name)>0:\n",
    "            park = re.findall(r\"NAME\\s([\\S\\s]*)\",line.getText())\n",
    "            \n",
    "            df_parks.loc[i] = [park[0],feature['geometry']['coordinates'][1],feature['geometry']['coordinates'][0]]\n",
    "            \n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parks.to_csv('../datasets/output/parks.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crematoria, Columbaria and Cemeteries\n",
    "\n",
    "source: data.gov.sg\n",
    "\n",
    "- scrape the data from the geojson file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a new dataframe for crematoria\n",
    "df_crematoria = pd.DataFrame(columns=['crematorium','lat','lng'])\n",
    "\n",
    "with open('../datasets/input/crematoria-geojson.geojson') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "i=0\n",
    "for feature in data['features']:\n",
    "    \n",
    "    soup = BeautifulSoup(feature['properties']['Description'], 'lxml')\n",
    "    \n",
    "    table = soup.find('table')\n",
    "       \n",
    "    for line in table.findAll('tr'):\n",
    "        \n",
    "        name = re.findall(r\"\\bNAME\\b\",line.getText())\n",
    "        \n",
    "        if len(name)>0:\n",
    "            crema = re.findall(r\"NAME\\s([\\S\\s]*)\",line.getText())\n",
    "            \n",
    "            df_crematoria.loc[i] = [crema[0],feature['geometry']['coordinates'][1],feature['geometry']['coordinates'][0]]\n",
    "            \n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crematorium</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kong Meng San Phor Kark See Monastery (Bright ...</td>\n",
       "      <td>1.361505</td>\n",
       "      <td>103.835805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tze Tho Aum Temple</td>\n",
       "      <td>1.361860</td>\n",
       "      <td>103.838299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Choa Chu Kang Crematorium</td>\n",
       "      <td>1.370578</td>\n",
       "      <td>103.686983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mandai Crematorium</td>\n",
       "      <td>1.413912</td>\n",
       "      <td>103.809632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         crematorium       lat         lng\n",
       "0  Kong Meng San Phor Kark See Monastery (Bright ...  1.361505  103.835805\n",
       "1                                Tze Tho Aum Temple   1.361860  103.838299\n",
       "2                         Choa Chu Kang Crematorium   1.370578  103.686983\n",
       "3                                Mandai Crematorium   1.413912  103.809632"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crematoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crematoria.to_csv('../datasets/output/crematoria.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe for crematoria\n",
    "df_columbaria = pd.DataFrame(columns=['columbaria','lat','lng'])\n",
    "\n",
    "with open('../datasets/input/dedicated-columbaria-geojson.geojson') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "i=0\n",
    "for feature in data['features']:\n",
    "    \n",
    "    soup = BeautifulSoup(feature['properties']['Description'], 'lxml')\n",
    "    \n",
    "    table = soup.find('table')\n",
    "       \n",
    "    for line in table.findAll('tr'):\n",
    "        \n",
    "        name = re.findall(r\"\\bNAME\\b\",line.getText())\n",
    "        \n",
    "        if len(name)>0:\n",
    "            colum = re.findall(r\"NAME\\s([\\S\\s]*)\",line.getText())\n",
    "            \n",
    "            df_columbaria.loc[i] = [colum[0],feature['geometry']['coordinates'][1],feature['geometry']['coordinates'][0]]\n",
    "            \n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columbaria.to_csv('../datasets/output/columbaria.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe for cemeteries\n",
    "df_cemeteries = pd.DataFrame(columns=['cemetery','lat','lng'])\n",
    "\n",
    "with open('../datasets/input/active-cemeteries-geojson.geojson') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "i=0\n",
    "for feature in data['features']:\n",
    "    \n",
    "    soup = BeautifulSoup(feature['properties']['Description'], 'lxml')\n",
    "    \n",
    "    table = soup.find('table')\n",
    "       \n",
    "    for line in table.findAll('tr'):\n",
    "        \n",
    "        name = re.findall(r\"\\bNAME\\b\",line.getText())\n",
    "        \n",
    "        if len(name)>0:\n",
    "            cem = re.findall(r\"NAME\\s([\\S\\s]*)\",line.getText())\n",
    "            \n",
    "            df_cemeteries.loc[i] = [cem[0],feature['geometry']['coordinates'][1],feature['geometry']['coordinates'][0]]\n",
    "            \n",
    "            i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cemetery</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chua Chu Kang Ahmadiyya Jama'at Cemetery</td>\n",
       "      <td>1.369400</td>\n",
       "      <td>103.688042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chua Chu Kang Bahai Cemetery</td>\n",
       "      <td>1.374913</td>\n",
       "      <td>103.692977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chua Chu Kang Chinese Cemetery</td>\n",
       "      <td>1.381858</td>\n",
       "      <td>103.686390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chua Chu Kang Christian Cemetery</td>\n",
       "      <td>1.373497</td>\n",
       "      <td>103.689689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chua Chu Kang Hindu Cemetery</td>\n",
       "      <td>1.369422</td>\n",
       "      <td>103.685886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chua Chu Kang Jewish Cemetery</td>\n",
       "      <td>1.371647</td>\n",
       "      <td>103.699983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chua Chu Kang Muslim Cemetery</td>\n",
       "      <td>1.383451</td>\n",
       "      <td>103.687554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chua Chu Kang Parsi Cemetery</td>\n",
       "      <td>1.371738</td>\n",
       "      <td>103.699554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lawn Cemetery</td>\n",
       "      <td>1.373497</td>\n",
       "      <td>103.689689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>State Cemetery, Kranji</td>\n",
       "      <td>1.419480</td>\n",
       "      <td>103.757139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    cemetery       lat         lng\n",
       "0  Chua Chu Kang Ahmadiyya Jama'at Cemetery   1.369400  103.688042\n",
       "1              Chua Chu Kang Bahai Cemetery   1.374913  103.692977\n",
       "2            Chua Chu Kang Chinese Cemetery   1.381858  103.686390\n",
       "3          Chua Chu Kang Christian Cemetery   1.373497  103.689689\n",
       "4              Chua Chu Kang Hindu Cemetery   1.369422  103.685886\n",
       "5             Chua Chu Kang Jewish Cemetery   1.371647  103.699983\n",
       "6             Chua Chu Kang Muslim Cemetery   1.383451  103.687554\n",
       "7              Chua Chu Kang Parsi Cemetery   1.371738  103.699554\n",
       "8                             Lawn Cemetery   1.373497  103.689689\n",
       "9                    State Cemetery, Kranji   1.419480  103.757139"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cemeteries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cemeteries.to_csv('../datasets/output/cemeteries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sports Facilities\n",
    "\n",
    "Source: data.gov.sg\n",
    "\n",
    "- scrape the data from the geojson file provided\n",
    "- convert categorical data to dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe for sports facilities\n",
    "df_sports = pd.DataFrame(columns=['road','facilities','lat','lng'])\n",
    "\n",
    "# Create an empty list to hold all facilities found\n",
    "facilities_list = []\n",
    "\n",
    "with open('../datasets/input/sportsg-sport-facilities-geojson.geojson') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "i=0\n",
    "for feature in data['features']:\n",
    "    \n",
    "    soup = BeautifulSoup(feature['properties']['Description'], 'lxml')\n",
    "    \n",
    "    table = soup.find('table')\n",
    "       \n",
    "    for line in table.findAll('tr'):\n",
    "        \n",
    "        \n",
    "        fa = re.findall(r\"\\bFACILITIES\\b\",line.getText())\n",
    "        if len(fa)>0:\n",
    "            fac = re.findall(r\"FACILITIES\\s([\\S\\s]*)\",line.getText())\n",
    "            \n",
    "            # add to facilities_list\n",
    "            facilities_list.extend(fac[0].lower().strip().split('/'))\n",
    "        \n",
    "        \n",
    "        rd = re.findall(r\"\\bROAD_NAME\\b\",line.getText())\n",
    "        if len(rd)>0:\n",
    "            road = re.findall(r\"ROAD_NAME\\s([\\S\\s]*)\",line.getText())\n",
    "            \n",
    "            #print(road[0])\n",
    "            \n",
    "            \n",
    "            gp = feature['geometry']['coordinates']\n",
    "\n",
    "        \n",
    "    if len(fac)>0 and len(road)>0 and g[0][1]>0:\n",
    "        df_sports.loc[i] = [road[0],fac[0].lower().strip(),gp[0][0][1],gp[0][0][0]]\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now facilities_list has collected all types of facilities, many of them are duplicates\n",
    "# So use set to extract unique types\n",
    "f_list=[]\n",
    "for i in facilities_list:\n",
    "    f_list.append(i.strip())\n",
    "    \n",
    "facilities_set = set(f_list)\n",
    "print(facilities_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the dummy variables, one for each type of facility\n",
    "for i in facilities_set:\n",
    "    df_sports[i] = 0\n",
    "\n",
    "for index, row in df_sports.iterrows():\n",
    "    f_list = row['facilities'].strip().split('/') # split the facilities string\n",
    "    \n",
    "    for j in f_list:\n",
    "        #set corresponding column to 1\n",
    "        if j in facilities_set:\n",
    "            df_sports.loc[index,j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sports.rename(columns={'gateball & petanque courts':'gateball_petanque_courts',\n",
    "                'swimming complex':'swimming_complex','squash centre':'squash_centre',\n",
    "                'netball centre':'netball_centre','lawn bowl':'lawn_bowl','sports hall':'sports_hall',\n",
    "                'futsal court':'futsal_court','tennis centre':'tennis_centre',\n",
    "                'practice track':'practice_track','hockey pitch':'hockey_pitch'\n",
    "                },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sports.drop(labels='facilities',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sports.to_csv('../datasets/output/sports.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
